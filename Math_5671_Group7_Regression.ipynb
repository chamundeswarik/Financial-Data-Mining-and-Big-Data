{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, pandas as pd, numpy as np\n",
    "from pandas import DataFrame\n",
    "from io import StringIO\n",
    "import time, json\n",
    "from datetime import date\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pylab as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reason why we have the getOrCreate code\n",
    "# http://stackoverflow.com/questions/28999332/how-to-access-sparkcontext-in-pyspark-script\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Adj_Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Stock_price_prediction').getOrCreate()\n",
    "df = spark.read.csv('AAPL_stock_Quandl.csv', header = True, inferSchema = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+--------------------+\n",
      "|summary|             Open|              High|               Low|         Adj_Close|              Volume|\n",
      "+-------+-----------------+------------------+------------------+------------------+--------------------+\n",
      "|  count|             9594|              9594|              9594|              9594|                9594|\n",
      "|   mean|24.55178285275794|24.785660443571015|24.301431815118807| 24.54790937998181|1.2427037658640817E7|\n",
      "| stddev| 45.1043238715968| 45.48898305164385|44.702582603777564|45.102037176071086| 1.687100863123329E7|\n",
      "|    min|      0.159815622|       0.159815622|       0.158090993|       0.158090993|                4471|\n",
      "|    max|      228.9938134|       231.6629934|       228.0015532|       230.2738291|           189560600|\n",
      "+-------+-----------------+------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('O-L', df['Open'] - df['Low'])\n",
    "df = df.withColumn('O-C', df.Open - df.Adj_Close)\n",
    "df = df.withColumn('H-L', df.High - df.Low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag, lit, lead\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('dummy_column', lit('Stocks!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.orderBy(\"dummy_column\")\n",
    "value_lead = lead('Adj_Close', count=10).over(w)\n",
    "df = df.withColumn('Next_Adj_Close', value_lead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Seasonality Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, date_format\n",
    "from pyspark.sql.functions import year, month\n",
    "from pyspark.sql.functions import dayofmonth, weekofyear, dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('DATEFORMAT', to_date('Date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('YEAR', year('DATEFORMAT'))\n",
    "df = df.withColumn('MONTH', month('DATEFORMAT'))\n",
    "df = df.withColumn('DAYOFMONTH', dayofmonth('DATEFORMAT'))\n",
    "df = df.withColumn('DAYOFMONTH', dayofmonth('DATEFORMAT'))\n",
    "df = df.withColumn('DAYOFWEEK', dayofweek('DATEFORMAT'))\n",
    "df = df.withColumn('WEEKOFYEAR', weekofyear('DATEFORMAT'))\n",
    "df = df.withColumn(\"week\", date_format('DATEFORMAT', \"W\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().orderBy(df['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, lit, col, lead, lag\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "\n",
    "def weighted_average(c, window, offsets, weights):\n",
    "    assert len(weights) == len(offsets)\n",
    "\n",
    "    def value(i):\n",
    "        if i < 0: return lag(c, -i).over(window)\n",
    "        if i > 0: return lead(c, i).over(window)\n",
    "        return c\n",
    "\n",
    "    values = [coalesce(value(i) * w, lit(0)) for i, w in zip(offsets, weights)]\n",
    "\n",
    "    return reduce(add, values, lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"dummy_column\").orderBy(\"DATEFORMAT\")\n",
    "# offsets, delays =  [-1,0], [0.5, 0.5]\n",
    "a = [1/20] *20\n",
    "offsets, delays =   [int(e) for e in list(np.arange(20)*-1)] , a\n",
    "df = df.withColumn(\"Avg_Close_20\", weighted_average(col(\"DAYOFMONTH\"), w, offsets, delays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1/10] *10\n",
    "offsets, delays =   [int(e) for e in list(np.arange(10)*-1)] , a\n",
    "df = df.withColumn(\"Avg_Close_10\", weighted_average(col(\"DAYOFMONTH\"), w, offsets, delays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1/5] *5\n",
    "offsets, delays =   [int(e) for e in list(np.arange(5)*-1)] , a\n",
    "df = df.withColumn(\"Avg_Close_5\", weighted_average(col(\"DAYOFMONTH\"), w, offsets, delays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1/80] *80\n",
    "offsets, delays =   [int(e) for e in list(np.arange(80)*-1)] , a\n",
    "df = df.withColumn(\"Avg_Close_80\", weighted_average(col(\"DAYOFMONTH\"), w, offsets, delays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets, delays =   [0, 1, 8, 15], [0.25,0.25,0.25,0.25]\n",
    "df = df.withColumn(\"Avg_Close_0_1_8_15\", weighted_average(col(\"DAYOFMONTH\"), w, offsets, delays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets, delays =   [0, 1, 3, 5], [0.25,0.25,0.25,0.25]\n",
    "df = df.withColumn(\"Avg_Close_0_1_3_5\", weighted_average(col(\"DAYOFMONTH\"), w, offsets, delays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets, delays =   [0, 1, 3, 5], [0.25,0.25,0.25,0.25]\n",
    "df = df.withColumn(\"Avg_Close_0_1_3_5\", weighted_average(col(\"DAYOFMONTH\"), w, offsets, delays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets, delays =   [0, 1, 5, 20, 80], [0.2,0.2,0.2,0.2,0.2]\n",
    "df = df.withColumn(\"Avg_Close_0_1_5_20_80\", weighted_average(col(\"DAYOFMONTH\"), w, offsets, delays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets, delays =   [0, 1, 2], [0.333,0.333,0.333]\n",
    "df = df.withColumn(\"Avg_Close_3\", weighted_average(col(\"DAYOFMONTH\"), w, offsets, delays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets, delays =   [0, 1], [0.5,0.5]\n",
    "df = df.withColumn(\"Avg_Close_2\", weighted_average(col(\"DAYOFMONTH\"), w, offsets, delays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Adj_Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- O-L: double (nullable = true)\n",
      " |-- dummy_column: string (nullable = false)\n",
      " |-- Next_Adj_Close: double (nullable = true)\n",
      " |-- DATEFORMAT: date (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAYOFMONTH: integer (nullable = true)\n",
      " |-- DAYOFWEEK: integer (nullable = true)\n",
      " |-- WEEKOFYEAR: integer (nullable = true)\n",
      " |-- week: string (nullable = true)\n",
      " |-- Avg_Close_20: double (nullable = false)\n",
      " |-- Avg_Close_10: double (nullable = false)\n",
      " |-- Avg_Close_5: double (nullable = false)\n",
      " |-- Avg_Close_80: double (nullable = false)\n",
      " |-- Avg_Close_0_1_8_15: double (nullable = false)\n",
      " |-- Avg_Close_0_1_3_5: double (nullable = false)\n",
      " |-- Avg_Close_0_1_5_20_80: double (nullable = false)\n",
      " |-- Avg_Close_3: double (nullable = false)\n",
      " |-- Avg_Close_2: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[cols] = df1[cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols = list(df1.columns.drop(['Date','Next_Adj_Close']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('final_data_2weeks.csv', header = True, inferSchema = True)\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembler = VectorAssembler(inputCols=inputCols, outputCol=\"features\")\n",
    "df = df_assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data for Train & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df = df.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.where(\"rank > .8\").drop(\"rank\")\n",
    "train_df = df.where(\"rank <= .8\").drop(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df=train_df.select(['features','Next_Adj_Close'])\n",
    "test_df=test_df.select(['features','Next_Adj_Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- Next_Adj_Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|            features|Next_Adj_Close|\n",
      "+--------------------+--------------+\n",
      "|[0.371801272,0.37...|   0.409599391|\n",
      "+--------------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lin_Reg=LinearRegression(labelCol='Next_Adj_Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lin_Reg.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.152734028777544"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26196493790931297,0.25649065029117035,0.26836888636629863,0.26242684905452435,2.0081525907301185e-09,0.30118745915321854,0.005678093969356122,-0.0033289035238435994,-0.010611173931461954,0.009928366024230046,0.0017398715152661785,0.08895654288664033,-0.022478078530639913,-0.0034150175142458107,0.0009118580230399939,0.061967872576272245,0.006000093492723331,0.006197580379800791,0.0017835013928192288,0.000922637516614471,-0.007989496329536097,-0.4872375944015094,-0.2142385342059671,0.005558242357116584,-0.13795933522645296,-0.09226942179169335,0.04608427787325021,0.06331251808076013,0.00837133986862244,0.0962730938815078,-0.015237715629850705,-0.36950274396457533,-0.7752821277517656,-0.39996915837749114,0.15682925949588186]\n"
     ]
    }
   ],
   "source": [
    "print(lr_model.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions=lr_model.evaluate(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38500370265179373"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_predictions.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995049883290615"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_predictions.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results=lr_model.evaluate(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "| -2.0107649347974714|\n",
      "| -2.2989881562630856|\n",
      "|  -2.933175019558533|\n",
      "|  -3.082065158504612|\n",
      "| -3.4125569273422656|\n",
      "| -3.5286413502111174|\n",
      "| -1.3318217029092523|\n",
      "| -1.5577026476689326|\n",
      "| -0.4189549525743175|\n",
      "|-0.34703610709799904|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#view the residual errors based on predictions \n",
    "test_results.residuals.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+\n",
      "|Next_Adj_Close|prediction       |\n",
      "+--------------+-----------------+\n",
      "|41.02800281   |43.03876774479747|\n",
      "|41.76163274   |44.06062089626309|\n",
      "|41.04684604   |43.98002105955853|\n",
      "|40.84710776   |43.92917291850461|\n",
      "|40.23156209   |43.64411901734226|\n",
      "|39.61099157   |43.13963292021112|\n",
      "|40.86469478   |42.19651648290925|\n",
      "|40.52677277   |42.08447541766893|\n",
      "|41.60963065   |42.02858560257432|\n",
      "|40.99659742   |41.343633527098  |\n",
      "+--------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test Set results\n",
    "\n",
    "results=lr_model.evaluate(test_df).predictions\n",
    "\n",
    "results.select(['Next_Adj_Close','prediction']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.regression.LinearRegressionSummary"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.982910168051495"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.785639449169558"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.47362383578703"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+------------------+\n",
      "|            features|Next_Adj_Close|        prediction|\n",
      "+--------------------+--------------+------------------+\n",
      "|[42.05809963,42.4...|   41.02800281| 43.03876774479747|\n",
      "|[42.84951549,43.6...|   41.76163274| 44.06062089626309|\n",
      "|[43.82559504,44.2...|   41.04684604| 43.98002105955853|\n",
      "|[43.52787193,43.7...|   40.84710776| 43.92917291850461|\n",
      "|[43.11080834,43.3...|   40.23156209| 43.64411901734226|\n",
      "|[43.31934014,43.5...|   39.61099157| 43.13963292021112|\n",
      "|[42.4814443,42.48...|   40.86469478| 42.19651648290925|\n",
      "|[41.67872251,42.0...|   40.52677277| 42.08447541766893|\n",
      "|[41.86338621,41.9...|   41.60963065| 42.02858560257432|\n",
      "|[41.52420799,41.6...|   40.99659742|   41.343633527098|\n",
      "|[41.10337575,41.2...|   41.71138412|41.290774744566576|\n",
      "|[41.45511613,41.8...|   42.11588555|  42.0806183659393|\n",
      "|[41.42371074,41.4...|   41.96262725| 41.47662680404716|\n",
      "|[41.06568928,41.2...|   42.16739039| 41.27144750777135|\n",
      "|[41.32823835,41.3...|   43.12085807| 40.95210886093309|\n",
      "|[39.86663145,39.9...|   43.89594312| 40.03738286796673|\n",
      "|[39.78183689,40.9...|   44.18864136| 41.12739238368402|\n",
      "|[40.84710776,41.3...|   44.87202267|41.174674512092984|\n",
      "|[40.06574163,41.6...|   45.18733279|41.852061464372426|\n",
      "|[41.62721767,41.8...|   44.47003366| 41.63697196142178|\n",
      "+--------------------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_lin = np.array(sorted(range(len(list((lr_model.coefficients)))), key=lambda i: list((lr_model.coefficients))[i], reverse=True)[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols = pd.Series(inputCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O-L', 'Low', 'Adj_Close', 'Open', 'High', 'StdClose_80', 'MaxClose_20', 'week', 'MaxClose_5', 'Avg_Close_80', 'MinClose_80', 'DAYOFWEEK', 'MaxClose_10', 'Avg_Close_0_1_3_5', 'Avg_Close_0_1_8_15']\n"
     ]
    }
   ],
   "source": [
    "print(list(inputCols[best_features_lin]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "rf = RandomForestRegressor(labelCol='Next_Adj_Close')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rf_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|Next_Adj_Close|prediction        |\n",
      "+--------------+------------------+\n",
      "|41.02800281   |41.94620892863519 |\n",
      "|41.76163274   |41.06625277648734 |\n",
      "|41.04684604   |37.958130722299764|\n",
      "|40.84710776   |38.05938084661707 |\n",
      "|40.23156209   |38.18338037732402 |\n",
      "|39.61099157   |38.4054204218328  |\n",
      "|40.86469478   |38.04074070697382 |\n",
      "|40.52677277   |37.954718894714674|\n",
      "|41.60963065   |37.9064421237059  |\n",
      "|40.99659742   |38.39565183656198 |\n",
      "+--------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_rf = rf_model.transform(test_df)\n",
    "\n",
    "results_rf.select(['Next_Adj_Close','prediction']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 77.9411\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Next_Adj_Close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(results)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = rf_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbr = GBTRegressor(labelCol='Next_Adj_Close')\n",
    "gbr_model = gbr.fit(training_df)\n",
    "results_gbr = gbr_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|Next_Adj_Close|prediction        |\n",
      "+--------------+------------------+\n",
      "|41.02800281   |42.54162418140933 |\n",
      "|41.76163274   |43.09764074468664 |\n",
      "|41.04684604   |42.66045155565485 |\n",
      "|40.84710776   |42.917523149026486|\n",
      "|40.23156209   |42.917523149026486|\n",
      "|39.61099157   |42.66045155565485 |\n",
      "|40.86469478   |41.79420747427775 |\n",
      "|40.52677277   |42.39503119893849 |\n",
      "|41.60963065   |42.41858608131081 |\n",
      "|40.99659742   |42.756618724460196|\n",
      "+--------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_gbr = gbr_model.transform(test_df)\n",
    "# test_results=gbr_model.transform(test_df)\n",
    "\n",
    "results_gbr.select(['Next_Adj_Close','prediction']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+------------------+\n",
      "|            features|Next_Adj_Close|        prediction|\n",
      "+--------------------+--------------+------------------+\n",
      "|[42.05809963,42.4...|   41.02800281| 41.94620892863519|\n",
      "|[42.84951549,43.6...|   41.76163274| 41.06625277648734|\n",
      "|[43.82559504,44.2...|   41.04684604|37.958130722299764|\n",
      "|[43.52787193,43.7...|   40.84710776| 38.05938084661707|\n",
      "|[43.11080834,43.3...|   40.23156209| 38.18338037732402|\n",
      "|[43.31934014,43.5...|   39.61099157|  38.4054204218328|\n",
      "|[42.4814443,42.48...|   40.86469478| 38.04074070697382|\n",
      "|[41.67872251,42.0...|   40.52677277|37.954718894714674|\n",
      "|[41.86338621,41.9...|   41.60963065|  37.9064421237059|\n",
      "|[41.52420799,41.6...|   40.99659742| 38.39565183656198|\n",
      "|[41.10337575,41.2...|   41.71138412| 38.22188856306198|\n",
      "|[41.45511613,41.8...|   42.11588555| 38.22188856306198|\n",
      "|[41.42371074,41.4...|   41.96262725| 38.22188856306198|\n",
      "|[41.06568928,41.2...|   42.16739039| 38.03097212170301|\n",
      "|[41.32823835,41.3...|   43.12085807| 38.03097212170301|\n",
      "|[39.86663145,39.9...|   43.89594312| 38.03097212170301|\n",
      "|[39.78183689,40.9...|   44.18864136| 38.39565183656198|\n",
      "|[40.84710776,41.3...|   44.87202267| 38.22188856306198|\n",
      "|[40.06574163,41.6...|   45.18733279| 38.03097212170301|\n",
      "|[41.62721767,41.8...|   44.47003366| 38.39565183656198|\n",
      "+--------------------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 76.1831\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Next_Adj_Close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(results_gbr)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[prediction: double]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gbr.select(['prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 75.9021\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'Next_Adj_Close')\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Next_Adj_Close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_best_features = list(dt_model.featureImportances)\n",
    "\n",
    "best_features = np.array(sorted(range(len(list((DT_best_features)))), key=lambda i: list((lr_model.coefficients))[i], reverse=True)[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  2,  3,  0,  1, 34, 29, 11, 27, 15, 26,  9, 28, 17, 16])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O-L', 'Low', 'Adj_Close', 'Open', 'High', 'StdClose_80', 'MaxClose_20', 'week', 'MaxClose_5', 'Avg_Close_80', 'MinClose_80', 'DAYOFWEEK', 'MaxClose_10', 'Avg_Close_0_1_3_5', 'Avg_Close_0_1_8_15']\n"
     ]
    }
   ],
   "source": [
    "print(list(inputCols[best_features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
