{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+---------+------+-------------------+-------------------+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|       payment_date|        last_update|\n",
      "+----------+-----------+--------+---------+------+-------------------+-------------------+\n",
      "|         1|          1|       1|       76|  2.99|2005-05-25 11:30:37|2006-02-15 22:12:30|\n",
      "|         2|          1|       1|      573|  0.99|2005-05-28 10:35:23|2006-02-15 22:12:30|\n",
      "|         3|          1|       1|     1185|  5.99|2005-06-15 00:54:12|2006-02-15 22:12:30|\n",
      "+----------+-----------+--------+---------+------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payment_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/payment.csv\")\n",
    "payment_df = payment_df.alias('payment_df')\n",
    "payment_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "|rental_id|        rental_date|inventory_id|customer_id|        return_date|staff_id|        last_update|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "|        1|2005-05-24 22:53:30|         367|        130|2005-05-26 22:04:30|       1|2006-02-15 21:30:53|\n",
      "|        2|2005-05-24 22:54:33|        1525|        459|2005-05-28 19:40:33|       1|2006-02-15 21:30:53|\n",
      "|        3|2005-05-24 23:03:39|        1711|        408|2005-06-01 22:12:39|       1|2006-02-15 21:30:53|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rental_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/rental.csv\")\n",
    "rental_df = rental_df.alias('rental_df')\n",
    "rental_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+-------+--------------------+--------+------+--------+--------------------+--------------+\n",
      "|staff_id|first_name|last_name|address_id|picture|               email|store_id|active|username|            password|   last_update|\n",
      "+--------+----------+---------+----------+-------+--------------------+--------+------+--------+--------------------+--------------+\n",
      "|       1|      Mike|  Hillyer|         3|   NULL|Mike.Hillyer@saki...|       1|     1|    Mike|8cb2237d0679ca88d...|2/15/2006 3:57|\n",
      "|       2|       Jon| Stephens|         4|   NULL|Jon.Stephens@saki...|       2|     1|     Jon|                NULL|2/15/2006 3:57|\n",
      "+--------+----------+---------+----------+-------+--------------------+--------+------+--------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staff_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/staff.csv\")\n",
    "staff_df = staff_df.alias('staff_df')\n",
    "staff_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+--------+-------------------+\n",
      "|inventory_id|film_id|store_id|        last_update|\n",
      "+------------+-------+--------+-------------------+\n",
      "|           1|      1|       1|2006-02-15 05:09:17|\n",
      "|           2|      1|       1|2006-02-15 05:09:17|\n",
      "|           3|      1|       1|2006-02-15 05:09:17|\n",
      "+------------+-------+--------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inventory_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/inventory.csv\")\n",
    "inventory_df = inventory_df.alias('inventory_df')\n",
    "inventory_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------+-------------------+\n",
      "|store_id|manager_staff_id|address_id|        last_update|\n",
      "+--------+----------------+----------+-------------------+\n",
      "|       1|               1|         1|2006-02-15 04:57:12|\n",
      "|       2|               2|         2|2006-02-15 04:57:12|\n",
      "+--------+----------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "store_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/store.csv\")\n",
    "store_df = store_df.alias('store_df')\n",
    "store_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------+--------+-------+-----------+-----------+-------------------+\n",
      "|address_id|           address|address2|district|city_id|postal_code|      phone|        last_update|\n",
      "+----------+------------------+--------+--------+-------+-----------+-----------+-------------------+\n",
      "|         1| 47 MySakila Drive|    NULL| Alberta|    300|       null|       null|2014-09-25 22:30:27|\n",
      "|         2|28 MySQL Boulevard|    NULL|     QLD|    576|       null|       null|2014-09-25 22:30:09|\n",
      "|         3| 23 Workhaven Lane|    NULL| Alberta|    300|       null|14033335568|2014-09-25 22:30:27|\n",
      "+----------+------------------+--------+--------+-------+-----------+-----------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "address_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/address.csv\")\n",
    "address_df = address_df.alias('address_df')\n",
    "address_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+---------+--------------------+----------+------+-------------------+-------------------+\n",
      "|customer_id|store_id|first_name|last_name|               email|address_id|active|        create_date|        last_update|\n",
      "+-----------+--------+----------+---------+--------------------+----------+------+-------------------+-------------------+\n",
      "|          1|       1|      MARY|    SMITH|MARY.SMITH@sakila...|         5|     1|2006-02-14 22:04:36|2006-02-15 04:57:20|\n",
      "|          2|       1|  PATRICIA|  JOHNSON|PATRICIA.JOHNSON@...|         6|     1|2006-02-14 22:04:36|2006-02-15 04:57:20|\n",
      "|          3|       1|     LINDA| WILLIAMS|LINDA.WILLIAMS@sa...|         7|     1|2006-02-14 22:04:36|2006-02-15 04:57:20|\n",
      "+-----------+--------+----------+---------+--------------------+----------+------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/customer.csv\")\n",
    "customer_df = customer_df.alias('customer_df')\n",
    "customer_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------------------+------------+-----------+--------------------+---------------+-----------+------+----------------+------+--------------------+-------------------+\n",
      "|film_id|           title|         description|release_year|language_id|original_language_id|rental_duration|rental_rate|length|replacement_cost|rating|    special_features|        last_update|\n",
      "+-------+----------------+--------------------+------------+-----------+--------------------+---------------+-----------+------+----------------+------+--------------------+-------------------+\n",
      "|      1|ACADEMY DINOSAUR|A Epic Drama of a...|        2006|          1|                NULL|              6|       0.99|    86|           20.99|    PG|Deleted Scenes,Be...|2006-02-15 05:03:42|\n",
      "|      2|  ACE GOLDFINGER|A Astounding Epis...|        2006|          1|                NULL|              3|       4.99|    48|           12.99|     G|Trailers,Deleted ...|2006-02-15 05:03:42|\n",
      "|      3|ADAPTATION HOLES|A Astounding Refl...|        2006|          1|                NULL|              7|       2.99|    50|           18.99| NC-17|Trailers,Deleted ...|2006-02-15 05:03:42|\n",
      "+-------+----------------+--------------------+------------+-----------+--------------------+---------------+-----------+------+----------------+------+--------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "film_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/film.csv\")\n",
    "film_df = film_df.alias('film_df')\n",
    "film_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|customer_id|       sum(amount)|\n",
      "+-----------+------------------+\n",
      "|        526| 221.5500000000001|\n",
      "|        148| 216.5400000000001|\n",
      "|        144|195.58000000000007|\n",
      "+-----------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 1\n",
    "#create data frame, agrregate amount and then sort in descending order\n",
    "totalamount_df= payment_df[['customer_id','amount']]\n",
    "desc_amount = totalamount_df.groupBy('customer_id')\n",
    "sorted_totalamount_df = desc_amount.agg({'amount':'sum'})\n",
    "sorted_totalamount_df.orderBy('sum(amount)', ascending = False).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+-------------+\n",
      "|rental_id|        rental_date|inventory_id|customer_id|        return_date|staff_id|        last_update|rental_date_1|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+-------------+\n",
      "|       32|2005-05-25 04:06:21|        3832|        230|2005-05-25 23:55:21|       1|2006-02-15 21:30:53|         2005|\n",
      "|       21|2005-05-25 01:59:46|         146|        388|2005-05-26 01:01:46|       2|2006-02-15 21:30:53|         2005|\n",
      "|       14|2005-05-25 00:31:15|        2701|        446|2005-05-26 02:56:15|       1|2006-02-15 21:30:53|         2005|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 2\n",
    "#create data frame, filter by year and then sort in ascending order\n",
    "rental1_df = rental_df.withColumn('rental_date_1', rental_df['rental_date'].substr(1, 4))\n",
    "rental2_df = rental1_df.filter(rental1_df.rental_date_1 ==2005)\n",
    "rental2_df.orderBy('return_date', ascending = True).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|staff_id|count|\n",
      "+--------+-----+\n",
      "|       1| 8040|\n",
      "|       2| 8004|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 3\n",
    "#create data frame of staff_id, rental id and group by staff_if. Count based on staff_id the attribute rental_id\n",
    "from pyspark.sql.functions import col\n",
    "staffid_df= rental_df[['staff_id','rental_id']]\n",
    "staffid = staffid_df.groupBy('staff_id')\n",
    "rentalid_count_df = staffid.agg({'rental_id':'count'})\n",
    "rentalid_count_df = rentalid_count_df.select(col(\"staff_id\").alias(\"staff_id\"), col(\"count(rental_id)\").alias(\"count\"))\n",
    "rentalid_count_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|first_name|last_name|count|\n",
      "+----------+---------+-----+\n",
      "|      Mike|  Hillyer| 8040|\n",
      "|       Jon| Stephens| 8004|\n",
      "+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 4\n",
    "#join data frames and extract specified columns\n",
    "name_df = rentalid_count_df.join(staff_df,(rentalid_count_df.staff_id == staff_df.staff_id))\n",
    "name_sorted_df= name_df[[['first_name','last_name', 'count']]]\n",
    "name_sorted_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5\n",
    "#creating new data frames \n",
    "Q_rental_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/rental.csv\")\n",
    "Q_address_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/address.csv\")\n",
    "Q_film_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/film.csv\")\n",
    "Q_customer_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/customer.csv\")\n",
    "Q_inventory_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/inventory.csv\")\n",
    "Q_payment_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/payment.csv\")\n",
    "Q_staff_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/staff.csv\")\n",
    "Q_store_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"D:/MATH 5671/Group 7/Assignment 9/data/store.csv\")\n",
    "\n",
    "\n",
    "#rental, inventory has inventory_id common, extracting store_id\n",
    "Xjoin_df = Q_rental_df.join(Q_inventory_df,(Q_rental_df.inventory_id == Q_inventory_df.inventory_id))\n",
    "#store , current (Ystore_df) has store id common, extracting address id\n",
    "Yjoin_df = Xjoin_df.join(Q_store_df, (Xjoin_df.store_id == Q_store_df.store_id), how ='left')\n",
    "#address , current (Yaddress_df) has address id common, extracting address\n",
    "Zjoin_df = Yjoin_df.join(Q_address_df,(Yjoin_df.address_id == Q_address_df.address_id))\n",
    "#customer, current (Zjoin_df) has customer_id in common\n",
    "Wjoin_df = Zjoin_df.join(Q_customer_df,(Zjoin_df.customer_id == Q_customer_df.customer_id))\n",
    "#staff, current (Wjoin_df) has store_id in common\n",
    "Vjoin_df = Wjoin_df.join(Q_staff_df, (Wjoin_df.staff_id == Q_staff_df.staff_id), how ='left')\n",
    "#film, current (Vjoin_df) has film_id in common\n",
    "Ujoin_df = Vjoin_df.join(Q_film_df, (Vjoin_df.film_id == Q_film_df.film_id))\n",
    "#since first_name, last_name are duplicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+----------+---------+----------+---------+---------------+--------------------+\n",
      "|        rental_date|        return_date|           address|first_name|last_name|first_name|last_name|          title|         description|\n",
      "+-------------------+-------------------+------------------+----------+---------+----------+---------+---------------+--------------------+\n",
      "|2005-05-24 22:53:30|2005-05-26 22:04:30| 47 MySakila Drive| CHARLOTTE|   HUNTER|      Mike|  Hillyer|BLANKET BEVERLY|A Emotional Docum...|\n",
      "|2005-05-24 22:54:33|2005-05-28 19:40:33|28 MySQL Boulevard|     TOMMY|  COLLAZO|      Mike|  Hillyer|   FREAKY POCUS|A Fast-Paced Docu...|\n",
      "|2005-05-24 23:03:39|2005-06-01 22:12:39|28 MySQL Boulevard|    MANUEL|  MURRELL|      Mike|  Hillyer|  GRADUATE LORD|A Lacklusture Epi...|\n",
      "+-------------------+-------------------+------------------+----------+---------+----------+---------+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_cols = [\n",
    "'rental_id',\n",
    " 'inventory_id',\n",
    " 'customer_id',\n",
    " 'staff_id',\n",
    " 'last_update',\n",
    " 'inventory_id',\n",
    " 'film_id',\n",
    " 'store_id',\n",
    " 'last_update',\n",
    " 'store_id',\n",
    " 'manager_staff_id',\n",
    " 'address_id',\n",
    " 'last_update',\n",
    " 'address_id',\n",
    " 'address2',\n",
    " 'district',\n",
    " 'city_id',\n",
    " 'postal_code',\n",
    " 'phone',\n",
    " 'last_update',\n",
    " 'customer_id',\n",
    " 'store_id',\n",
    " 'email',\n",
    " 'address_id',\n",
    " 'active',\n",
    " 'create_date',\n",
    " 'last_update',\n",
    " 'staff_id',\n",
    " 'address_id',\n",
    " 'picture',\n",
    " 'email',\n",
    " 'store_id',\n",
    " 'active',\n",
    " 'username',\n",
    " 'password',\n",
    " 'last_update',\n",
    " 'film_id',\n",
    " 'release_year',\n",
    " 'language_id',\n",
    " 'original_language_id',\n",
    " 'rental_duration',\n",
    " 'rental_rate',\n",
    " 'length',\n",
    " 'replacement_cost',\n",
    " 'rating',\n",
    " 'special_features',\n",
    " 'last_update']\n",
    "Ujoin_df = Ujoin_df.drop(*drop_cols)\n",
    "Ujoin_df.orderBy('rental_date', ascending = True).show(3) #Image in assignment pdf is ordered on 'rental_date'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+----------+---------+----------+---------+------------------+--------------------+\n",
      "|        rental_date|        return_date|           address|first_name|last_name|first_name|last_name|             title|         description|\n",
      "+-------------------+-------------------+------------------+----------+---------+----------+---------+------------------+--------------------+\n",
      "|2005-05-25 04:06:21|2005-05-25 23:55:21|28 MySQL Boulevard|       JOY|   GEORGE|      Mike|  Hillyer| STALLION SUNDANCE|A Fast-Paced Tale...|\n",
      "|2005-05-25 01:59:46|2005-05-26 01:01:46| 47 MySakila Drive|     CRAIG|  MORRELL|       Jon| Stephens|     APACHE DIVINE|A Awe-Inspiring R...|\n",
      "|2005-05-25 00:31:15|2005-05-26 02:56:15|28 MySQL Boulevard|  THEODORE|     CULP|      Mike|  Hillyer|MONTEREY LABYRINTH|A Awe-Inspiring D...|\n",
      "+-------------------+-------------------+------------------+----------+---------+----------+---------+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ujoin_df.orderBy('return_date', ascending = True).show(3) #ordered on 'return_date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|customer_id|      sum(amount)|\n",
      "+-----------+-----------------+\n",
      "|        526|221.5500000000001|\n",
      "|        148|216.5400000000001|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 6\n",
    "payment1_df = payment_df.withColumn('payment_date_1', payment_df['payment_date'].substr(1, 4))\n",
    "# we can replace 2005 with any year we require to filter\n",
    "payment2_df = payment1_df.filter(payment1_df.payment_date_1 ==2005)\n",
    "greaterpmt_df= payment2_df[['customer_id','amount','payment_date_1']]\n",
    "greaterpmt_df = greaterpmt_df.groupBy('customer_id')\n",
    "sorted_greaterpmt_df = greaterpmt_df.agg({'amount':'sum'})\n",
    "sorted_greaterpmt_df = sorted_greaterpmt_df.select(col(\"customer_id\").alias(\"customer_id\"), col(\"sum(amount)\").alias(\"amount\"))\n",
    "sorted_greaterpmt1_df= sorted_greaterpmt_df.filter(sorted_greaterpmt_df.amount > 200.0)\n",
    "sorted_greaterpmt1_df = sorted_greaterpmt1_df.select(col(\"customer_id\").alias(\"customer_id\"), col(\"amount\").alias(\"sum(amount)\"))\n",
    "sorted_greaterpmt1_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+\n",
      "|first_name|last_name|      sum(amount)|\n",
      "+----------+---------+-----------------+\n",
      "|      KARL|     SEAL|221.5500000000001|\n",
      "|   ELEANOR|     HUNT|216.5400000000001|\n",
      "+----------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 7\n",
    "#join data frames and get specified columns\n",
    "customername_df = sorted_greaterpmt1_df.join(customer_df,(sorted_greaterpmt1_df.customer_id == customer_df.customer_id))\n",
    "customername_sorted_df= customername_df[[['first_name','last_name', 'sum(amount)']]]\n",
    "customername_sorted_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 8\n",
    "#create two data frames and subtract one data frame from another to get unique values \n",
    "rental5_df = rental_df.withColumn('rental_date_1', rental_df['rental_date'].substr(1, 4))\n",
    "customer2005= rental5_df.filter(rental5_df.rental_date_1 ==2005)\n",
    "customer2005_df= customer2005[['customer_id']]\n",
    "customer2006 = rental5_df.filter(rental5_df.rental_date_1 ==2006)\n",
    "customer2006_df= customer2006[['customer_id']]\n",
    "customer2005only_df = customer2005_df.subtract(customer2006_df)\n",
    "customer2005only_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer2006only_df = customer2006_df.subtract(customer2005_df)\n",
    "customer2006only_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
